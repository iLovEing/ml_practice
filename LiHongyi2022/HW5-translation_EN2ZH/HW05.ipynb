{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFEKWoh3p1Mv"
   },
   "source": [
    "# Homework Description\n",
    "- English to Chinese (Traditional) Translation\n",
    "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
    "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
    "\n",
    "- TODO\n",
    "    - Train a simple RNN seq2seq to acheive translation\n",
    "    - Switch to transformer model to boost performance\n",
    "    - Apply Back-translation to furthur boost performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Vf1Q79XPQ3D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 22 21:55:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:26:00.0  On |                  N/A |\n",
      "| 42%   44C    P8    19W / 125W |   1164MiB /  6144MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       932    C+G   ...zf8qxf38zg5c\\SkypeApp.exe    N/A      |\n",
      "|    0   N/A  N/A      1624    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5408    C+G   ...er_engine\\wallpaper32.exe    N/A      |\n",
      "|    0   N/A  N/A      5964    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      6108    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6812    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      7188    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      7236    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A      8432    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10060    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     11292    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     11624    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59neB_Sxp5Ub"
   },
   "source": [
    "# Download and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRlFbfFRpZYT"
   },
   "outputs": [],
   "source": [
    "!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
    "!pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSksMTdmp-Wt"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/fairseq.git\n",
    "!cd fairseq && git checkout 9a1c497\n",
    "!pip install --upgrade ./fairseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uRLTiuIuqGNc"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import pprint\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from fairseq import utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n07Za1XqJzA"
   },
   "source": [
    "# Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xllxxyWxqI7s"
   },
   "outputs": [],
   "source": [
    "seed = 19961111\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5ORDJ-2qdYw"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "## En-Zh Bilingual Parallel Corpus\n",
    "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
    "    - Raw: 398,066 (sentences)   \n",
    "    - Processed: 393,980 (sentences)\n",
    "    \n",
    "\n",
    "## Testdata\n",
    "- Size: 4,000 (sentences)\n",
    "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQw2mY4Dqkzd"
   },
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXT42xQtqijD"
   },
   "source": [
    "data_dir = './DATA/rawdata'\n",
    "dataset_name = 'ted2020'\n",
    "urls = (\n",
    "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted2020.tgz\",\n",
    "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/test.tgz\",\n",
    ")\n",
    "file_names = (\n",
    "    'ted2020.tgz', # train & dev\n",
    "    'test.tgz', # test\n",
    ")\n",
    "prefix = Path(data_dir).absolute() / dataset_name\n",
    "\n",
    "prefix.mkdir(parents=True, exist_ok=True)\n",
    "for u, f in zip(urls, file_names):\n",
    "    path = prefix/f\n",
    "    if not path.exists():\n",
    "        !wget {u} -O {path}\n",
    "    if path.suffix == \".tgz\":\n",
    "        !tar -xvf {path} -C {prefix}\n",
    "    elif path.suffix == \".zip\":\n",
    "        !unzip -o {path} -d {prefix}\n",
    "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
    "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
    "!mv {prefix/'test/test.en'} {prefix/'test.raw.en'}\n",
    "!mv {prefix/'test/test.zh'} {prefix/'test.raw.zh'}\n",
    "!rm -rf {prefix/'test'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLkJwNiFrIwZ"
   },
   "source": [
    "## Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_uJYkCncrKJb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('E:\\\\project\\\\python\\\\ml_pratice\\\\DATA\\\\rawdata\\\\ted2020/train_dev.raw',\n",
       " 'E:\\\\project\\\\python\\\\ml_pratice\\\\DATA\\\\rawdata\\\\ted2020/test.raw')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './DATA/rawdata'\n",
    "dataset_name = 'ted2020'\n",
    "prefix = Path(data_dir).absolute() / dataset_name\n",
    "\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'zh'\n",
    "\n",
    "data_prefix = f'{prefix}/train_dev.raw'\n",
    "test_prefix = f'{prefix}/test.raw'\n",
    "data_prefix, test_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0t2CPt1brOT3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much, Chris.\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "And I say that sincerely, partly because  I need that.\n",
      "Put yourselves in my position.\n",
      "闈炲父璎濊瑵浣狅紝鍏嬮噷鏂�銆傝兘鏈夐�欏�嬫�熸渻绗�浜屽害韪忎笂閫欏�嬫紨璎涘彴\n",
      "鐪熸槸涓�澶фΞ骞搞�傛垜闈炲父鎰熸縺銆�\n",
      "閫欏�嬬爺瑷庢渻绲︽垜鐣欎笅浜嗘サ鐐烘繁鍒荤殑鍗拌薄锛屾垜鎯虫劅璎濆ぇ瀹� 灏嶆垜涔嬪墠婕旇瑳鐨勫ソ瑭曘��\n",
      "鎴戞槸鐢辫》鐨勬兂閫欓杭瑾�锛屾湁閮ㄤ唤鍘熷洜鏄�鍥犵偤 鈥斺�� 鎴戠湡鐨勬湁闇�瑕�!\n",
      "璜嬩綘鍊戣ō韬�铏曞湴鐐烘垜鎯充竴鎯筹紒\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.'+src_lang} -n 5\n",
    "!head {data_prefix+'.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRoE9UK7r1gY"
   },
   "source": [
    "## Preprocess files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3tzFwtnFrle3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"Full width -> half width\"\"\"\n",
    "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
    "    ss = []\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # Full width space: direct conversion\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss.append(rstring)\n",
    "    return ''.join(ss)\n",
    "                \n",
    "def clean_s(s, lang):\n",
    "    if lang == 'en':\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace('-', '') # remove '-'\n",
    "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
    "    elif lang == 'zh':\n",
    "        s = strQ2B(s) # Q2B\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace(' ', '')\n",
    "        s = s.replace('—', '')\n",
    "        s = s.replace('“', '\"')\n",
    "        s = s.replace('”', '\"')\n",
    "        s = s.replace('_', '')\n",
    "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
    "    s = ' '.join(s.strip().split())\n",
    "    return s\n",
    "\n",
    "def len_s(s, lang):\n",
    "    if lang == 'zh':\n",
    "        return len(s)\n",
    "    return len(s.split())\n",
    "\n",
    "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
    "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
    "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
    "        return\n",
    "    with open(f'{prefix}.{l1}', 'r', encoding = 'utf-8') as l1_in_f:\n",
    "        with open(f'{prefix}.{l2}', 'r', encoding = 'utf-8') as l2_in_f:\n",
    "            with open(f'{prefix}.clean.{l1}', 'w', encoding = 'utf-8') as l1_out_f:\n",
    "                with open(f'{prefix}.clean.{l2}', 'w', encoding = 'utf-8') as l2_out_f:\n",
    "                    for s1 in l1_in_f:\n",
    "                        s1 = s1.strip()\n",
    "                        s2 = l2_in_f.readline().strip()\n",
    "                        s1 = clean_s(s1, l1)\n",
    "                        s2 = clean_s(s2, l2)\n",
    "                        s1_len = len_s(s1, l1)\n",
    "                        s2_len = len_s(s2, l2)\n",
    "                        if min_len > 0: # remove short sentence\n",
    "                            if s1_len < min_len or s2_len < min_len:\n",
    "                                continue\n",
    "                        if max_len > 0: # remove long sentence\n",
    "                            if s1_len > max_len or s2_len > max_len:\n",
    "                                continue\n",
    "                        if ratio > 0: # remove by ratio of length\n",
    "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
    "                                continue\n",
    "                        print(s1, file=l1_out_f)\n",
    "                        print(s2, file=l2_out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h_i8b1PRr9Nf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
    "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gjT3XCy9r_rj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much , Chris .\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
      "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
      "And I say that sincerely , partly because I need that .\n",
      "Put yourselves in my position .\n",
      "闈炲父璎濊瑵浣� , 鍏嬮噷鏂� 銆� 鑳芥湁閫欏�嬫�熸渻绗�浜屽害韪忎笂閫欏�嬫紨璎涘彴\n",
      "鐪熸槸涓�澶фΞ骞� 銆� 鎴戦潪甯告劅婵� 銆�\n",
      "閫欏�嬬爺瑷庢渻绲︽垜鐣欎笅浜嗘サ鐐烘繁鍒荤殑鍗拌薄 , 鎴戞兂鎰熻瑵澶у�跺皪鎴戜箣鍓嶆紨璎涚殑濂借�� 銆�\n",
      "鎴戞槸鐢辫》鐨勬兂閫欓杭瑾� , 鏈夐儴浠藉師鍥犳槸鍥犵偤鎴戠湡鐨勬湁闇�瑕� !\n",
      "璜嬩綘鍊戣ō韬�铏曞湴鐐烘垜鎯充竴鎯� !\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
    "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKb4u67-sT_Z"
   },
   "source": [
    "## Split into train/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AuFKeDz3sGHL"
   },
   "outputs": [],
   "source": [
    "valid_ratio = 0.01 # 3000~4000 would suffice\n",
    "train_ratio = 1 - valid_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QR2NVldqsXyY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/valid splits exists. skipping split.\n"
     ]
    }
   ],
   "source": [
    "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
    "    print(f'train/valid splits exists. skipping split.')\n",
    "else:\n",
    "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}', encoding = 'utf-8'))\n",
    "    labels = list(range(line_num))\n",
    "    random.shuffle(labels)\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w', encoding = 'utf-8')\n",
    "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w', encoding = 'utf-8')\n",
    "        count = 0\n",
    "        for line in open(f'{data_prefix}.clean.{lang}', 'r', encoding = 'utf-8'):\n",
    "            if labels[count]/line_num < train_ratio:\n",
    "                train_f.write(line)\n",
    "            else:\n",
    "                valid_f.write(line)\n",
    "            count += 1\n",
    "        train_f.close()\n",
    "        valid_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1rwQysTsdJq"
   },
   "source": [
    "## Subword Units \n",
    "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
    "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
    "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ecwllsa7sZRA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020/spm8000.model exists. skipping spm_train.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 8000\n",
    "if (prefix/f'spm{vocab_size}.model').exists():\n",
    "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
    "else:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
    "                        f'{prefix}/valid.clean.{src_lang}',\n",
    "                        f'{prefix}/train.clean.{tgt_lang}',\n",
    "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
    "        model_prefix=prefix/f'spm{vocab_size}',\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=1,\n",
    "        model_type='unigram', # 'bpe' works as well\n",
    "        input_sentence_size=1e6,\n",
    "        shuffle_input_sentence=True,\n",
    "        normalization_rule_name='nmt_nfkc_cf',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lQPRNldqse_V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020\\train.en exists. skipping spm_encode.\n",
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020\\train.zh exists. skipping spm_encode.\n",
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020\\valid.en exists. skipping spm_encode.\n",
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020\\valid.zh exists. skipping spm_encode.\n",
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020\\test.en exists. skipping spm_encode.\n",
      "E:\\project\\python\\ml_pratice\\DATA\\rawdata\\ted2020\\test.zh exists. skipping spm_encode.\n"
     ]
    }
   ],
   "source": [
    "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
    "in_tag = {\n",
    "    'train': 'train.clean',\n",
    "    'valid': 'valid.clean',\n",
    "    'test': 'test.raw.clean',\n",
    "}\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        out_path = prefix/f'{split}.{lang}'\n",
    "        if out_path.exists():\n",
    "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
    "        else:\n",
    "            with open(prefix/f'{split}.{lang}', 'w', encoding = 'utf-8') as out_f:\n",
    "                with open(prefix/f'{in_tag[split]}.{lang}', 'r', encoding = 'utf-8') as in_f:\n",
    "                    for line in in_f:\n",
    "                        line = line.strip()\n",
    "                        tok = spm_model.encode(line, out_type=str)\n",
    "                        print(' '.join(tok), file=out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4j6lXHjAsjXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鈻乼hank 鈻亂ou 鈻乻o 鈻乵uch 鈻�, 鈻乧hris 鈻�.\n",
      "鈻乤nd 鈻乮t ' s 鈻� t ru ly 鈻乤 鈻乬reat 鈻乭o n or 鈻乼o 鈻乭ave 鈻乼he 鈻� op port un ity 鈻乼o 鈻乧ome 鈻乼o 鈻乼his 鈻乻t age 鈻� t wi ce 鈻�; 鈻乮 ' m 鈻乪x t re me ly 鈻乬r ate ful 鈻�.\n",
      "鈻乮 鈻乭ave 鈻乥een 鈻� bl ow n 鈻乤way 鈻乥y 鈻乼his 鈻乧on fer ence 鈻�, 鈻乤nd 鈻乮 鈻亀ant 鈻乼o 鈻乼hank 鈻乤ll 鈻乷f 鈻亂ou 鈻乫or 鈻乼he 鈻乵any 鈻� ni ce 鈻� com ment s 鈻乤bout 鈻亀hat 鈻乮 鈻乭ad 鈻乼o 鈻乻ay 鈻乼he 鈻乷ther 鈻乶ight 鈻�.\n",
      "鈻乤nd 鈻乮 鈻乻ay 鈻乼hat 鈻乻ince re ly 鈻�, 鈻乸art ly 鈻乥ecause 鈻乮 鈻乶eed 鈻乼hat 鈻�.\n",
      "鈻乸ut 鈻亂our s el ve s 鈻乮n 鈻乵y 鈻乸o s ition 鈻�.\n",
      "鈻� 闈炲父 璎� 璎� 浣� 鈻�, 鈻� 鍏� 閲� 鏂� 鈻併�� 鈻� 鑳� 鏈� 閫欏�� 姗熸渻 绗�浜� 搴� 韪� 涓� 閫欏�� 婕旇瑳 鍙�\n",
      "鈻� 鐪� 鏄� 涓� 澶� 姒� 骞� 鈻併�� 鈻佹垜 闈炲父 鎰� 婵� 鈻併��\n",
      "鈻侀�欏�� 鐮� 瑷� 鏈� 绲︽垜 鐣� 涓� 浜� 妤� 鐐� 娣� 鍒� 鐨� 鍗� 璞� 鈻�, 鈻佹垜鎯� 鎰� 璎� 澶у�� 灏嶆垜 涔嬪墠 婕旇瑳 鐨� 濂� 瑭� 鈻併��\n",
      "鈻佹垜 鏄�鐢� 琛� 鐨� 鎯� 閫欓杭 瑾� 鈻�, 鈻佹湁 閮ㄤ唤 鍘熷洜 鏄�鍥犵偤 鎴� 鐪熺殑 鏈� 闇�瑕� 鈻�!\n",
      "鈻� 璜� 浣犲�� 瑷� 韬� 铏� 鍦� 鐐� 鎴戞兂 涓� 鎯� 鈻�!\n"
     ]
    }
   ],
   "source": [
    "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
    "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59si_C0Wsms7"
   },
   "source": [
    "## Binarize the data with fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w-cHVLSpsknh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA\\data-bin\\ted2020 exists, will not overwrite!\n"
     ]
    }
   ],
   "source": [
    "binpath = Path('./DATA/data-bin', dataset_name)\n",
    "if binpath.exists():\n",
    "    print(binpath, \"exists, will not overwrite!\")\n",
    "else:\n",
    "    !python -m fairseq_cli.preprocess \\\n",
    "        --source-lang {src_lang}\\\n",
    "        --target-lang {tgt_lang}\\\n",
    "        --trainpref {prefix/'train'}\\\n",
    "        --validpref {prefix/'valid'}\\\n",
    "        --testpref {prefix/'test'}\\\n",
    "        --destdir {binpath}\\\n",
    "        --joined-dictionary\\\n",
    "        --workers 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szMuH1SWLPWA"
   },
   "source": [
    "# Configuration for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5Luz3_tVLUxs"
   },
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    datadir = \"./DATA/data-bin/ted2020\",\n",
    "    savedir = \"./checkpoints/rnn\",\n",
    "    source_lang = \"en\",\n",
    "    target_lang = \"zh\",\n",
    "    \n",
    "    # cpu threads when fetching & processing data.\n",
    "    num_workers=8,  \n",
    "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
    "    max_tokens=8192,\n",
    "    accum_steps=2,\n",
    "    \n",
    "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
    "    lr_factor=2.,\n",
    "    lr_warmup=4000,\n",
    "    \n",
    "    # clipping gradient norm helps alleviate gradient exploding\n",
    "    clip_norm=1.0,\n",
    "    \n",
    "    # maximum epochs for training\n",
    "    max_epoch=30,\n",
    "    start_epoch=1,\n",
    "    \n",
    "    # beam size for beam search\n",
    "    beam=5, \n",
    "    # generate sequences of maximum length ax + b, where x is the source length\n",
    "    max_len_a=1.2, \n",
    "    max_len_b=10, \n",
    "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
    "    post_process = \"sentencepiece\",\n",
    "    \n",
    "    # checkpoints\n",
    "    keep_last_epochs=5,\n",
    "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
    "    \n",
    "    # logging\n",
    "    use_wandb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjrJFvyQLg86"
   },
   "source": [
    "# Logging\n",
    "- logging package logs ordinary messages\n",
    "- wandb logs the loss, bleu, etc. in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-ZiMyDWALbDk"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "proj = \"hw5.seq2seq\"\n",
    "logger = logging.getLogger(proj)\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNoSkK45Lmqc"
   },
   "source": [
    "# CUDA Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oqrsbmcoLqMl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:37:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-26 09:37:18 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 6.000 GB ; name = NVIDIA GeForce GTX 1660 SUPER           \n",
      "2022-11-26 09:37:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
     ]
    }
   ],
   "source": [
    "cuda_env = utils.CudaEnvironment()\n",
    "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbJuBIHLLt2D"
   },
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpG4EBRLwe_"
   },
   "source": [
    "## We borrow the TranslationTask from fairseq\n",
    "* used to load the binarized data created above\n",
    "* well-implemented data iterator (dataloader)\n",
    "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
    "* well-implemented beach search decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3gSEy1uFLvVs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:37:18 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
      "2022-11-26 09:37:18 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
     ]
    }
   ],
   "source": [
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=config.datadir,\n",
    "    source_lang=config.source_lang,\n",
    "    target_lang=config.target_lang,\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    "    upsample_primary=1,\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mR7Bhov7L4IU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:37:19 | INFO | hw5.seq2seq | loading data for epoch 1\n",
      "2022-11-26 09:37:19 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020\\train.en-zh.en\n",
      "2022-11-26 09:37:19 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020\\train.en-zh.zh\n",
      "2022-11-26 09:37:19 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train en-zh 390041 examples\n",
      "2022-11-26 09:37:19 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020\\valid.en-zh.en\n",
      "2022-11-26 09:37:19 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020\\valid.en-zh.zh\n",
      "2022-11-26 09:37:19 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid en-zh 3939 examples\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"loading data for epoch 1\")\n",
    "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
    "task.load_dataset(split=\"valid\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "P0BCEm_9L6ig"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1,\n",
      " 'source': tensor([2035,   14,   16,   18,  620,  345,    8,  497,   16,  785,    8,  377,\n",
      "        1635,   29,   33,    2]),\n",
      " 'target': tensor([ 140,  317,  463, 3314, 2876, 1102,  433,   33,    2])}\n",
      "\"Source: isn't that putting the cart before the horse ?\"\n",
      "'Target: 這不是本末倒置嗎 ?'\n"
     ]
    }
   ],
   "source": [
    "sample = task.dataset(\"valid\")[1]\n",
    "pprint.pprint(sample)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['source'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcfCVa2FMBSE"
   },
   "source": [
    "# Dataset iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBvc-B_6MKZM"
   },
   "source": [
    "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
    "* Shuffles the training set for every epoch\n",
    "* Ignore sentences exceeding maximum length\n",
    "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
    "* Add eos and shift one token\n",
    "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
    "    - generally, prepending bos to the target would do the job (as shown below)\n",
    "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
    "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
    "    ```\n",
    "    # output target (target) and Decoder input (prev_output_tokens): \n",
    "                   eos = 2\n",
    "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
    "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OWFJFmCnMDXW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:37:21 | WARNING | fairseq.tasks.fairseq_task | 2,606 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[2612, 2278, 3157, 2967, 750, 2254, 3582, 427, 3376, 3741]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': tensor([806]),\n",
       " 'nsentences': 1,\n",
       " 'ntokens': 16,\n",
       " 'net_input': {'src_tokens': tensor([[ 22,  14,  71, 276, 714, 500,   8, 330, 471, 113,   8, 701, 358, 471,\n",
       "             7,   2]]),\n",
       "  'src_lengths': tensor([16]),\n",
       "  'prev_output_tokens': tensor([[   2,   53,  437,  190,  402, 3440,  154,   67, 2317, 3440,  154,  125,\n",
       "            300,  359,   34,   10]])},\n",
       " 'target': tensor([[  53,  437,  190,  402, 3440,  154,   67, 2317, 3440,  154,  125,  300,\n",
       "           359,   34,   10,    2]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=max_tokens,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=seed,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "        disable_iterator_cache=not cached,\n",
    "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
    "        # first call of this method has no effect. \n",
    "    )\n",
    "    return batch_iterator\n",
    "\n",
    "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
    "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
    "sample = next(demo_iter)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p86K-0g7Me4M"
   },
   "source": [
    "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
    "```python\n",
    "batch = {\n",
    "    \"id\": id, # id for each example \n",
    "    \"nsentences\": len(samples), # batch size (sentences)\n",
    "    \"ntokens\": ntokens, # batch size (tokens)\n",
    "    \"net_input\": {\n",
    "        \"src_tokens\": src_tokens, # sequence in source language\n",
    "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
    "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
    "    },\n",
    "    \"target\": target, # target sequence\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EyDBE5ZMkFZ"
   },
   "source": [
    "# Model Architecture\n",
    "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Hzh74qLIMfW_"
   },
   "outputs": [],
   "source": [
    "from fairseq.models import (\n",
    "    FairseqEncoder, \n",
    "    FairseqIncrementalDecoder,\n",
    "    FairseqEncoderDecoderModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI46v1z7MotH"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn0wSeLLMrbc"
   },
   "source": [
    "- The Encoder is a RNN or Transformer Encoder. The following description is for RNN. For every input token, Encoder will generate a output vector and a hidden states vector, and the hidden states vector is passed on to the next step. In other words, the Encoder sequentially reads in the input sequence, and outputs a single vector at each timestep, then finally outputs the final hidden states, or content vector, at the last timestep.\n",
    "- Parameters:\n",
    "  - *args*\n",
    "      - encoder_embed_dim: the dimension of embeddings, this compresses the one-hot vector into fixed dimensions, which achieves dimension reduction\n",
    "      - encoder_ffn_embed_dim is the dimension of hidden states and output vectors\n",
    "      - encoder_layers is the number of layers for Encoder RNN\n",
    "      - dropout determines the probability of a neuron's activation being set to 0, in order to prevent overfitting. Generally this is applied in training, and removed in testing.\n",
    "  - *dictionary*: the dictionary provided by fairseq. it's used to obtain the padding index, and in turn the encoder padding mask. \n",
    "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
    "\n",
    "- Inputs: \n",
    "    - *src_tokens*: integer sequence representing english e.g. 1, 28, 29, 205, 2 \n",
    "- Outputs: \n",
    "    - *outputs*: the output of RNN at each timestep, can be furthur processed by Attention\n",
    "    - *final_hiddens*: the hidden states of each timestep, will be passed to decoder for decoding\n",
    "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WcX3W4iGMq-S"
   },
   "outputs": [],
   "source": [
    "class RNNEncoder(FairseqEncoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        self.embed_dim = args.encoder_embed_dim\n",
    "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
    "        self.num_layers = args.encoder_layers\n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.padding_idx = dictionary.pad()\n",
    "        \n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
    "        return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "    def forward(self, src_tokens, **unused):\n",
    "        bsz, seqlen = src_tokens.size()\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # pass thru bidirectional RNN\n",
    "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
    "        x, final_hiddens = self.rnn(x, h0)\n",
    "        outputs = self.dropout_out_module(x)\n",
    "        # outputs = [sequence len, batch size, hid dim * directions]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        \n",
    "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
    "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
    "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
    "        \n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
    "        return tuple(\n",
    "            (\n",
    "                outputs,  # seq_len x batch x hidden\n",
    "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
    "                encoder_padding_mask,  # seq_len x batch\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
    "        return tuple(\n",
    "            (\n",
    "                encoder_out[0].index_select(1, new_order),\n",
    "                encoder_out[1].index_select(1, new_order),\n",
    "                encoder_out[2].index_select(1, new_order),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZlE_1JnMv56"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSFSKt_ZMzgh"
   },
   "source": [
    "- When the input sequence is long, \"content vector\" alone cannot accurately represent the whole sequence, attention mechanism can provide the Decoder more information.\n",
    "- According to the **Decoder embeddings** of the current timestep, match the **Encoder outputs** with decoder embeddings to determine correlation, and then sum the Encoder outputs weighted by the correlation as the input to **Decoder** RNN.\n",
    "- Common attention implementations use neural network / dot product as the correlation between **query** (decoder embeddings) and **key** (Encoder outputs), followed by **softmax**  to obtain a distribution, and finally **values** (Encoder outputs) is **weighted sum**-ed by said distribution.\n",
    "\n",
    "- Parameters:\n",
    "  - *input_embed_dim*: dimensionality of key, should be that of the vector in decoder to attend others\n",
    "  - *source_embed_dim*: dimensionality of query, should be that of the vector to be attended to (encoder outputs)\n",
    "  - *output_embed_dim*: dimensionality of value, should be that of the vector after attention, expected by the next layer\n",
    "\n",
    "- Inputs: \n",
    "    - *inputs*: is the key, the vector to attend to others\n",
    "    - *encoder_outputs*:  is the query/value, the vector to be attended to\n",
    "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n",
    "- Outputs: \n",
    "    - *output*: the context vector after attention\n",
    "    - *attention score*: the attention distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1Atf_YuCMyyF"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(\n",
    "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
    "        # inputs: T, B, dim\n",
    "        # encoder_outputs: S x B x dim\n",
    "        # padding mask:  S x B\n",
    "        \n",
    "        # convert all to batch first\n",
    "        inputs = inputs.transpose(1,0) # B, T, dim\n",
    "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
    "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
    "        \n",
    "        # project to the dimensionality of encoder_outputs\n",
    "        x = self.input_proj(inputs)\n",
    "\n",
    "        # compute attention\n",
    "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
    "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
    "\n",
    "        # cancel the attention at positions corresponding to padding\n",
    "        if encoder_padding_mask is not None:\n",
    "            # leveraging broadcast  B, S -> (B, 1, S)\n",
    "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
    "            attn_scores = (\n",
    "                attn_scores.float()\n",
    "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
    "                .type_as(attn_scores)\n",
    "            )  # FP16 support: cast to float and back\n",
    "\n",
    "        # softmax on the dimension corresponding to source sequence\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
    "        x = torch.bmm(attn_scores, encoder_outputs)\n",
    "\n",
    "        # (B, T, dim)\n",
    "        x = torch.cat((x, inputs), dim=-1)\n",
    "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
    "        \n",
    "        # restore shape (B, T, dim) -> (T, B, dim)\n",
    "        return x.transpose(1,0), attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doSCOA2gM7fK"
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M8Vod2gNABR"
   },
   "source": [
    "* The hidden states of **Decoder** will be initialized by the final hidden states of **Encoder** (the content vector)\n",
    "* At the same time, **Decoder** will change its hidden states based on the input of the current timestep (the outputs of previous timesteps), and generates an output\n",
    "* Attention improves the performance\n",
    "* The seq2seq steps are implemented in decoder, so that later the Seq2Seq class can accept RNN and Transformer, without furthur modification.\n",
    "- Parameters:\n",
    "  - *args*\n",
    "      - decoder_embed_dim: is the dimensionality of the decoder embeddings, similar to encoder_embed_dim，\n",
    "      - decoder_ffn_embed_dim: is the dimensionality of the decoder RNN hidden states, similar to encoder_ffn_embed_dim\n",
    "      - decoder_layers: number of layers of RNN decoder\n",
    "      - share_decoder_input_output_embed: usually, the projection matrix of the decoder will share weights with the decoder input embeddings\n",
    "  - *dictionary*: the dictionary provided by fairseq\n",
    "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
    "- Inputs: \n",
    "    - *prev_output_tokens*: integer sequence representing the right-shifted target e.g. 1, 28, 29, 205, 2 \n",
    "    - *encoder_out*: encoder's output.\n",
    "    - *incremental_state*: in order to speed up decoding during test time, we will save the hidden state of each timestep. see forward() for details.\n",
    "- Outputs: \n",
    "    - *outputs*: the logits (before softmax) output of decoder for each timesteps\n",
    "    - *extra*: unsused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QfvgqHYDM6Lp"
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(FairseqIncrementalDecoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
    "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
    "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
    "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
    "        \n",
    "        self.embed_dim = args.decoder_embed_dim\n",
    "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
    "        self.num_layers = args.decoder_layers\n",
    "        \n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.attention = AttentionLayer(\n",
    "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
    "        ) \n",
    "        # self.attention = None\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        if self.hidden_dim != self.embed_dim:\n",
    "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.project_out_dim = None\n",
    "        \n",
    "        if args.share_decoder_input_output_embed:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.embed_tokens.weight.shape[1],\n",
    "                self.embed_tokens.weight.shape[0],\n",
    "                bias=False,\n",
    "            )\n",
    "            self.output_projection.weight = self.embed_tokens.weight\n",
    "        else:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.output_embed_dim, len(dictionary), bias=False\n",
    "            )\n",
    "            nn.init.normal_(\n",
    "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
    "            )\n",
    "        \n",
    "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
    "        # extract the outputs from encoder\n",
    "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
    "        # outputs:          seq_len x batch x num_directions*hidden\n",
    "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
    "        # padding_mask:     seq_len x batch\n",
    "        \n",
    "        if incremental_state is not None and len(incremental_state) > 0:\n",
    "            # if the information from last timestep is retained, we can continue from there instead of starting from bos\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        else:\n",
    "            # incremental state does not exist, either this is training time, or the first timestep of test time\n",
    "            # prepare for seq2seq: pass the encoder_hidden to the decoder hidden states\n",
    "            prev_hiddens = encoder_hiddens\n",
    "        \n",
    "        bsz, seqlen = prev_output_tokens.size()\n",
    "        \n",
    "        # embed tokens\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "                \n",
    "        # decoder-to-encoder attention\n",
    "        if self.attention is not None:\n",
    "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
    "                        \n",
    "        # pass thru unidirectional RNN\n",
    "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
    "        # outputs = [sequence len, batch size, hid dim]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        x = self.dropout_out_module(x)\n",
    "                \n",
    "        # project to embedding size (if hidden differs from embed size, and share_embedding is True, \n",
    "        # we need to do an extra projection)\n",
    "        if self.project_out_dim != None:\n",
    "            x = self.project_out_dim(x)\n",
    "        \n",
    "        # project to vocab size\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(1, 0)\n",
    "        \n",
    "        # if incremental, record the hidden states of current timestep, which will be restored in the next timestep\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": final_hiddens,\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        \n",
    "        return x, None\n",
    "    \n",
    "    def reorder_incremental_state(\n",
    "        self,\n",
    "        incremental_state,\n",
    "        new_order,\n",
    "    ):\n",
    "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
    "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDAPmxjRNEEL"
   },
   "source": [
    "## Seq2Seq\n",
    "- Composed of **Encoder** and **Decoder**\n",
    "- Recieves inputs and pass to **Encoder** \n",
    "- Pass the outputs from **Encoder** to **Decoder**\n",
    "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
    "- Once done decoding, return the **Decoder** outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oRwKdLa0NEU6"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, args, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        prev_output_tokens,\n",
    "        return_all_hiddens: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the forward pass for an encoder-decoder model.\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "        logits, extra = self.decoder(\n",
    "            prev_output_tokens,\n",
    "            encoder_out=encoder_out,\n",
    "            src_lengths=src_lengths,\n",
    "            return_all_hiddens=return_all_hiddens,\n",
    "        )\n",
    "        return logits, extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu3C2JfqNHzk"
   },
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nyI9FOx-NJ2m"
   },
   "outputs": [],
   "source": [
    "# # HINT: transformer architecture\n",
    "from fairseq.models.transformer import (\n",
    "    TransformerEncoder, \n",
    "    TransformerDecoder,\n",
    ")\n",
    "\n",
    "def build_model(args, task):\n",
    "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
    "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "\n",
    "    # token embeddings\n",
    "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
    "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
    "    \n",
    "    # encoder decoder\n",
    "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
    "#     encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
    "#     decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "\n",
    "    # sequence to sequence model\n",
    "    model = Seq2Seq(args, encoder, decoder)\n",
    "    \n",
    "    # initialization for seq2seq model is important, requires extra handling\n",
    "    def init_params(module):\n",
    "        from fairseq.modules import MultiheadAttention\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, MultiheadAttention):\n",
    "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.RNNBase):\n",
    "            for name, param in module.named_parameters():\n",
    "                if \"weight\" in name or \"bias\" in name:\n",
    "                    param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    # weight initialization\n",
    "    model.apply(init_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce5n4eS7NQNy"
   },
   "source": [
    "## Architecture Related Configuration\n",
    "\n",
    "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Cyn30VoGNT6N"
   },
   "outputs": [],
   "source": [
    "arch_args = Namespace(\n",
    "    encoder_embed_dim=256,\n",
    "    encoder_ffn_embed_dim=512,\n",
    "    encoder_layers=4,\n",
    "    decoder_embed_dim=256,\n",
    "    decoder_ffn_embed_dim=1024,\n",
    "    decoder_layers=4,\n",
    "    share_decoder_input_output_embed=True,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "# HINT: these patches on parameters for Transformer\n",
    "def add_transformer_args(args):\n",
    "    args.encoder_attention_heads=4\n",
    "    args.encoder_normalize_before=True\n",
    "    \n",
    "    args.decoder_attention_heads=4\n",
    "    args.decoder_normalize_before=True\n",
    "    \n",
    "    args.activation_fn=\"relu\"\n",
    "    args.max_source_positions=1024\n",
    "    args.max_target_positions=1024\n",
    "    \n",
    "    # patches on default parameters for Transformer (those not set above)\n",
    "    from fairseq.models.transformer import base_architecture\n",
    "    base_architecture(arch_args)\n",
    "\n",
    "add_transformer_args(arch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Nbb76QLCNZZZ"
   },
   "outputs": [],
   "source": [
    "if config.use_wandb:\n",
    "    wandb.config.update(vars(arch_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7ZWfxsCDNatH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:37:25 | INFO | hw5.seq2seq | Seq2Seq(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=256, out_features=8000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(arch_args, task)\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHll7GRNNdqc"
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUB9f1WCNgMH"
   },
   "source": [
    "## Loss: Label Smoothing Regularization\n",
    "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
    "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
    "* avoids overfitting\n",
    "\n",
    "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "IgspdJn0NdYF"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
    "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, lprobs, target):\n",
    "        if target.dim() == lprobs.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
    "        # equivalent to summing the log probs of all labels\n",
    "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "        if self.ignore_index is not None:\n",
    "            pad_mask = target.eq(self.ignore_index)\n",
    "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "        else:\n",
    "            nll_loss = nll_loss.squeeze(-1)\n",
    "            smooth_loss = smooth_loss.squeeze(-1)\n",
    "        if self.reduce:\n",
    "            nll_loss = nll_loss.sum()\n",
    "            smooth_loss = smooth_loss.sum()\n",
    "        # when calculating cross-entropy, add the loss of other labels\n",
    "        eps_i = self.smoothing / lprobs.size(-1)\n",
    "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss\n",
    "\n",
    "# generally, 0.1 is good enough\n",
    "criterion = LabelSmoothedCrossEntropyCriterion(\n",
    "    smoothing=0.1,\n",
    "    ignore_index=task.target_dictionary.pad(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRalDto2NkJJ"
   },
   "source": [
    "## Optimizer: Adam + lr scheduling\n",
    "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
    "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
    "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "sS7tQj1ROBYm"
   },
   "outputs": [],
   "source": [
    "def get_rate(d_model, step_num, warmup_step):\n",
    "    # TODO: Change lr from constant to the equation shown above\n",
    "#     lr = 0.001\n",
    "    lr = np.power(d_model, -0.5) * min(np.power(step_num, -0.5), step_num * np.power(warmup_step, -1.5))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "J8hoAjHPNkh3"
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "        \n",
    "    def multiply_grads(self, c):\n",
    "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.mul_(c)\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFJlkOMONsc6"
   },
   "source": [
    "## Scheduling Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "A135fwPCNrQs"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = NoamOpt(\n",
    "    model_size=arch_args.encoder_embed_dim, \n",
    "    factor=config.lr_factor, \n",
    "    warmup=config.lr_warmup, \n",
    "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
    "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
    "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOR0g-cVO5ZO"
   },
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-0ZjbK3O8Iv"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "foal3xM1O404"
   },
   "outputs": [],
   "source": [
    "from fairseq.data import iterators\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
    "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
    "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
    "    \n",
    "    stats = {\"loss\": []}\n",
    "    scaler = GradScaler() # automatic mixed precision (amp) \n",
    "    \n",
    "    model.train()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
    "    for samples in progress:\n",
    "        model.zero_grad()\n",
    "        accum_loss = 0\n",
    "        sample_size = 0\n",
    "        # gradient accumulation: update every accum_steps samples\n",
    "        for i, sample in enumerate(samples):\n",
    "            if i == 1:\n",
    "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size_i = sample[\"ntokens\"]\n",
    "            sample_size += sample_size_i\n",
    "            \n",
    "            # mixed precision training\n",
    "            with autocast():\n",
    "                net_output = model.forward(**sample[\"net_input\"])\n",
    "                lprobs = F.log_softmax(net_output[0], -1)            \n",
    "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
    "                \n",
    "                # logging\n",
    "                accum_loss += loss.item()\n",
    "                # back-prop\n",
    "                scaler.scale(loss).backward()                \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
    "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # logging\n",
    "        loss_print = accum_loss/sample_size\n",
    "        stats[\"loss\"].append(loss_print)\n",
    "        progress.set_postfix(loss=loss_print)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss_print,\n",
    "                \"train/grad_norm\": gnorm.item(),\n",
    "                \"train/lr\": optimizer.rate(),\n",
    "                \"train/sample_size\": sample_size,\n",
    "            })\n",
    "        \n",
    "    loss_print = np.mean(stats[\"loss\"])\n",
    "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt1lX3DRO_yU"
   },
   "source": [
    "## Validation & Inference\n",
    "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
    "- the procedure is essensially same as training, with the addition of inference step\n",
    "- after validation we can save the model weights\n",
    "\n",
    "Validation loss alone cannot describe the actual performance of the model\n",
    "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
    "- We can also manually examine the hypotheses' quality\n",
    "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "2og80HYQPAKq"
   },
   "outputs": [],
   "source": [
    "# fairseq's beam search generator\n",
    "# given model and input seqeunce, produce translation hypotheses by beam search\n",
    "sequence_generator = task.build_generator([model], config)\n",
    "\n",
    "def decode(toks, dictionary):\n",
    "    # convert from Tensor to human readable sentence\n",
    "    s = dictionary.string(\n",
    "        toks.int().cpu(),\n",
    "        config.post_process,\n",
    "    )\n",
    "    return s if s else \"<unk>\"\n",
    "\n",
    "def inference_step(sample, model):\n",
    "    gen_out = sequence_generator.generate([model], sample)\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    for i in range(len(gen_out)):\n",
    "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
    "        srcs.append(decode(\n",
    "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
    "            task.source_dictionary,\n",
    "        ))\n",
    "        hyps.append(decode(\n",
    "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "        refs.append(decode(\n",
    "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "    return srcs, hyps, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "y1o7LeDkPDsd"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sacrebleu\n",
    "\n",
    "def validate(model, task, criterion, log_to_wandb=True):\n",
    "    logger.info('begin validation')\n",
    "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    \n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            net_output = model.forward(**sample[\"net_input\"])\n",
    "\n",
    "            lprobs = F.log_softmax(net_output[0], -1)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size = sample[\"ntokens\"]\n",
    "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
    "            progress.set_postfix(valid_loss=loss.item())\n",
    "            stats[\"loss\"].append(loss)\n",
    "            \n",
    "            # do inference\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            srcs.extend(s)\n",
    "            hyps.extend(h)\n",
    "            refs.extend(r)\n",
    "            \n",
    "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
    "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
    "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
    "    stats[\"srcs\"] = srcs\n",
    "    stats[\"hyps\"] = hyps\n",
    "    stats[\"refs\"] = refs\n",
    "    \n",
    "    if config.use_wandb and log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"valid/loss\": stats[\"loss\"],\n",
    "            \"valid/bleu\": stats[\"bleu\"].score,\n",
    "        }, commit=False)\n",
    "    \n",
    "    showid = np.random.randint(len(hyps))\n",
    "    logger.info(\"example source: \" + srcs[showid])\n",
    "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
    "    logger.info(\"example reference: \" + refs[showid])\n",
    "    \n",
    "    # show bleu results\n",
    "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
    "    logger.info(stats[\"bleu\"].format())\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sRF6nd4PGEE"
   },
   "source": [
    "# Save and Load Model Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "edBuLlkuPGr9"
   },
   "outputs": [],
   "source": [
    "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
    "    stats = validate(model, task, criterion)\n",
    "    bleu = stats['bleu']\n",
    "    loss = stats['loss']\n",
    "    if save:\n",
    "        # save epoch checkpoints\n",
    "        savedir = Path(config.savedir).absolute()\n",
    "        savedir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        check = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
    "            \"optim\": {\"step\": optimizer._step}\n",
    "        }\n",
    "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
    "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
    "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
    "    \n",
    "        # save epoch samples\n",
    "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\", encoding = 'utf-8') as f:\n",
    "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
    "                f.write(f\"{s}\\t{h}\\n\")\n",
    "\n",
    "        # get best valid bleu    \n",
    "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
    "            validate_and_save.best_bleu = bleu.score\n",
    "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
    "            \n",
    "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
    "        if del_file.exists():\n",
    "            del_file.unlink()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def try_load_checkpoint(model, optimizer=None, name=None):\n",
    "    name = name if name else \"checkpoint_last.pt\"\n",
    "    checkpath = Path(config.savedir)/name\n",
    "    if checkpath.exists():\n",
    "        check = torch.load(checkpath)\n",
    "        model.load_state_dict(check[\"model\"])\n",
    "        stats = check[\"stats\"]\n",
    "        step = \"unknown\"\n",
    "        if optimizer != None:\n",
    "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
    "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
    "    else:\n",
    "        logger.info(f\"no checkpoints found at {checkpath}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyIFpibfPJ5u"
   },
   "source": [
    "# Main\n",
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hu7RZbCUPKQr"
   },
   "outputs": [],
   "source": [
    "model = model.to(device=device)\n",
    "criterion = criterion.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5xxlJxU2PeAo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:37:40 | INFO | hw5.seq2seq | task: TranslationTask\n",
      "2022-11-26 09:37:40 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
      "2022-11-26 09:37:40 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
      "2022-11-26 09:37:40 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-11-26 09:37:40 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
      "2022-11-26 09:37:40 | INFO | hw5.seq2seq | num. model params: 12,523,520 (num. trained: 12,523,520)\n",
      "2022-11-26 09:37:40 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
    "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
    "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
    "logger.info(\n",
    "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    )\n",
    ")\n",
    "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "MSPRqpQUPfaX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:37:41 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326685]\n",
      "2022-11-26 09:37:41 | INFO | hw5.seq2seq | no checkpoints found at checkpoints\\rnn\\checkpoint_last.pt!\n",
      "2022-11-26 09:37:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 1:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:07:28 | INFO | hw5.seq2seq | training loss: 7.0524\n",
      "2022-11-26 10:07:28 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:08:17 | INFO | hw5.seq2seq | example source: i actually , i'm very afraid of sharks .\n",
      "2022-11-26 10:08:17 | INFO | hw5.seq2seq | example hypothesis: 我說 , 我很棒 。\n",
      "2022-11-26 10:08:17 | INFO | hw5.seq2seq | example reference: 實際上 , 我非常害怕鯊魚 。\n",
      "2022-11-26 10:08:17 | INFO | hw5.seq2seq | validation loss:\t5.9412\n",
      "2022-11-26 10:08:17 | INFO | hw5.seq2seq | BLEU = 1.01 18.7/3.1/0.6/0.1 (BP = 0.744 ratio = 0.772 hyp_len = 88075 ref_len = 114152)\n",
      "2022-11-26 10:08:17 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint1.pt\n",
      "2022-11-26 10:08:17 | INFO | hw5.seq2seq | end of epoch 1\n",
      "2022-11-26 10:08:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 2:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:34:18 | INFO | hw5.seq2seq | training loss: 5.5021\n",
      "2022-11-26 10:34:18 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:34:44 | INFO | hw5.seq2seq | example source: rives: so the first hour of this project was satisfying .\n",
      "2022-11-26 10:34:44 | INFO | hw5.seq2seq | example hypothesis: 克:第一個計畫 , 首先 , 首先 。\n",
      "2022-11-26 10:34:44 | INFO | hw5.seq2seq | example reference: rives:這個計劃開始後第一個小時的成果讓人很滿意 。\n",
      "2022-11-26 10:34:44 | INFO | hw5.seq2seq | validation loss:\t4.9527\n",
      "2022-11-26 10:34:44 | INFO | hw5.seq2seq | BLEU = 7.86 35.2/13.4/5.4/2.2 (BP = 0.906 ratio = 0.910 hyp_len = 103898 ref_len = 114152)\n",
      "2022-11-26 10:34:45 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint2.pt\n",
      "2022-11-26 10:34:45 | INFO | hw5.seq2seq | end of epoch 2\n",
      "2022-11-26 10:34:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 3:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:58:00 | INFO | hw5.seq2seq | training loss: 4.9079\n",
      "2022-11-26 10:58:00 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:58:23 | INFO | hw5.seq2seq | example source: afrigator is an aggregator of african blogs that was developed in south africa .\n",
      "2022-11-26 10:58:23 | INFO | hw5.seq2seq | example hypothesis: 阿富汗是非洲的組織之一 。\n",
      "2022-11-26 10:58:23 | INFO | hw5.seq2seq | example reference: afrigator是一個非洲的部落格平台它是在南非被開發的\n",
      "2022-11-26 10:58:23 | INFO | hw5.seq2seq | validation loss:\t4.5216\n",
      "2022-11-26 10:58:23 | INFO | hw5.seq2seq | BLEU = 10.27 45.6/19.9/9.4/4.6 (BP = 0.728 ratio = 0.759 hyp_len = 86657 ref_len = 114152)\n",
      "2022-11-26 10:58:23 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint3.pt\n",
      "2022-11-26 10:58:23 | INFO | hw5.seq2seq | end of epoch 3\n",
      "2022-11-26 10:58:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 4:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 11:21:49 | INFO | hw5.seq2seq | training loss: 4.6190\n",
      "2022-11-26 11:21:49 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 11:22:13 | INFO | hw5.seq2seq | example source: how many people told him he couldn't do that , that he would die if he tried that ?\n",
      "2022-11-26 11:22:13 | INFO | hw5.seq2seq | example hypothesis: 有多少人告訴我他無法這麼做 , 他會死掉 ?\n",
      "2022-11-26 11:22:13 | INFO | hw5.seq2seq | example reference: 有多少人告訴過他他做不到 , 如果他做了可能會死 ?\n",
      "2022-11-26 11:22:13 | INFO | hw5.seq2seq | validation loss:\t4.3029\n",
      "2022-11-26 11:22:13 | INFO | hw5.seq2seq | BLEU = 13.28 47.8/22.4/11.1/5.8 (BP = 0.820 ratio = 0.834 hyp_len = 95209 ref_len = 114152)\n",
      "2022-11-26 11:22:13 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint4.pt\n",
      "2022-11-26 11:22:13 | INFO | hw5.seq2seq | end of epoch 4\n",
      "2022-11-26 11:22:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 5:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 11:45:45 | INFO | hw5.seq2seq | training loss: 4.4409\n",
      "2022-11-26 11:45:45 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 11:46:09 | INFO | hw5.seq2seq | example source: i think it is absolutely wrong for research to begin in the first place without a clear plan for what would happen to the participants once the trial has ended .\n",
      "2022-11-26 11:46:09 | INFO | hw5.seq2seq | example hypothesis: 我認為 , 首先 , 研究從來沒有明顯的計畫 , 一旦試驗結束時 , 會發生什麼事 。\n",
      "2022-11-26 11:46:09 | INFO | hw5.seq2seq | example reference: 我認為一開始就沒有明確的計畫這個研究是絕對錯誤的因為也不知道一旦臨床試驗結束參與試驗者會發生什麼事\n",
      "2022-11-26 11:46:09 | INFO | hw5.seq2seq | validation loss:\t4.1347\n",
      "2022-11-26 11:46:09 | INFO | hw5.seq2seq | BLEU = 15.24 48.4/23.1/11.8/6.3 (BP = 0.896 ratio = 0.901 hyp_len = 102866 ref_len = 114152)\n",
      "2022-11-26 11:46:10 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint5.pt\n",
      "2022-11-26 11:46:10 | INFO | hw5.seq2seq | end of epoch 5\n",
      "2022-11-26 11:46:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 6:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 12:09:46 | INFO | hw5.seq2seq | training loss: 4.2969\n",
      "2022-11-26 12:09:46 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 12:10:09 | INFO | hw5.seq2seq | example source: and i chose two people to be , and i asked them to send me descriptions of how to act as them on facebook .\n",
      "2022-11-26 12:10:09 | INFO | hw5.seq2seq | example hypothesis: 我選擇兩個人 , 去做 , 我要求他們把我送到臉書上 。\n",
      "2022-11-26 12:10:09 | INFO | hw5.seq2seq | example reference: 後來我選擇了其中二個人 , 我想變成他們 , 所以我請他們告訴我要怎麼樣在facebook上扮演他們的角色 ,\n",
      "2022-11-26 12:10:09 | INFO | hw5.seq2seq | validation loss:\t3.9887\n",
      "2022-11-26 12:10:09 | INFO | hw5.seq2seq | BLEU = 17.14 50.1/24.9/13.0/7.1 (BP = 0.929 ratio = 0.932 hyp_len = 106350 ref_len = 114152)\n",
      "2022-11-26 12:10:09 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint6.pt\n",
      "2022-11-26 12:10:09 | INFO | hw5.seq2seq | end of epoch 6\n",
      "2022-11-26 12:10:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 7:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 12:34:34 | INFO | hw5.seq2seq | training loss: 4.1634\n",
      "2022-11-26 12:34:34 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 12:35:00 | INFO | hw5.seq2seq | example source: he had all these old automotive parts lying around .\n",
      "2022-11-26 12:35:00 | INFO | hw5.seq2seq | example hypothesis: 他有這些自動派對 。\n",
      "2022-11-26 12:35:00 | INFO | hw5.seq2seq | example reference: 他有很多這些舊的汽車零件撒滿一地 。\n",
      "2022-11-26 12:35:00 | INFO | hw5.seq2seq | validation loss:\t3.9118\n",
      "2022-11-26 12:35:00 | INFO | hw5.seq2seq | BLEU = 17.54 54.4/27.8/14.9/8.4 (BP = 0.842 ratio = 0.853 hyp_len = 97395 ref_len = 114152)\n",
      "2022-11-26 12:35:00 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint7.pt\n",
      "2022-11-26 12:35:00 | INFO | hw5.seq2seq | end of epoch 7\n",
      "2022-11-26 12:35:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 8:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 12:59:20 | INFO | hw5.seq2seq | training loss: 4.0746\n",
      "2022-11-26 12:59:20 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 12:59:44 | INFO | hw5.seq2seq | example source: and this time , my wish was not to photograph anymore just one animal that i had photographed all my life: us .\n",
      "2022-11-26 12:59:44 | INFO | hw5.seq2seq | example hypothesis: 這次 , 我的願望不是要拍攝任何的動物 , 我只拍攝我所有的動物:我們 。\n",
      "2022-11-26 12:59:44 | INFO | hw5.seq2seq | example reference: 這次 , 我的願望不再是拍攝我至今一直拍攝的物種:我們人類 。\n",
      "2022-11-26 12:59:44 | INFO | hw5.seq2seq | validation loss:\t3.8278\n",
      "2022-11-26 12:59:44 | INFO | hw5.seq2seq | BLEU = 18.65 54.4/28.2/15.4/8.8 (BP = 0.874 ratio = 0.881 hyp_len = 100571 ref_len = 114152)\n",
      "2022-11-26 12:59:44 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint8.pt\n",
      "2022-11-26 12:59:44 | INFO | hw5.seq2seq | end of epoch 8\n",
      "2022-11-26 12:59:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 9:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:23:37 | INFO | hw5.seq2seq | training loss: 4.0040\n",
      "2022-11-26 13:23:37 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:24:00 | INFO | hw5.seq2seq | example source: but the data didn't look like a planet .\n",
      "2022-11-26 13:24:00 | INFO | hw5.seq2seq | example hypothesis: 但資料並不像一個星球 。\n",
      "2022-11-26 13:24:00 | INFO | hw5.seq2seq | example reference: 但那資料看起來並不像個行星 。\n",
      "2022-11-26 13:24:00 | INFO | hw5.seq2seq | validation loss:\t3.7707\n",
      "2022-11-26 13:24:00 | INFO | hw5.seq2seq | BLEU = 19.87 53.5/28.0/15.4/8.9 (BP = 0.935 ratio = 0.937 hyp_len = 106956 ref_len = 114152)\n",
      "2022-11-26 13:24:00 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint9.pt\n",
      "2022-11-26 13:24:00 | INFO | hw5.seq2seq | end of epoch 9\n",
      "2022-11-26 13:24:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 10:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:47:38 | INFO | hw5.seq2seq | training loss: 3.9496\n",
      "2022-11-26 13:47:38 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:48:01 | INFO | hw5.seq2seq | example source: the problem is that , although there are many good scientists working on what they think are human pheromones , and they're publishing in respectable journals , at the basis of this , despite very sophisticated experiments , there really is no good science behind it , because it's based on a problem , which is nobody has systematically gone through all the odors that humans produce and there are thousands of molecules that we give off .\n",
      "2022-11-26 13:48:01 | INFO | hw5.seq2seq | example hypothesis: 問題是 , 雖然有許多很好的科學家在研究他們認為人類的phone , 他們在尊重的記者發表的報導中 , 儘管這個實驗的基礎上 , 卻沒有好處的科學 , 因為它是基於人類的體系 , 沒有人會透過數千個人類來製造出的分子 。\n",
      "2022-11-26 13:48:01 | INFO | hw5.seq2seq | example reference: 問題就在於 , 即使有很多傑出科學家在研究他們所認為的人類費洛蒙 , 即使他們在具權威性的期刊中出版有關論文 , 即使研究過程中涉及到非常複雜的實驗 , 人類費洛蒙的研究背後始終沒有正規科學得以支持 , 因為它是建基於另一個問題 , 那就是 , 至今未曾有人有系統地研究過人類產生的所有氣味 , 而我們所放出的這些氣味分子達至數以千計 。\n",
      "2022-11-26 13:48:01 | INFO | hw5.seq2seq | validation loss:\t3.7404\n",
      "2022-11-26 13:48:01 | INFO | hw5.seq2seq | BLEU = 20.66 54.1/28.6/15.8/9.1 (BP = 0.950 ratio = 0.952 hyp_len = 108624 ref_len = 114152)\n",
      "2022-11-26 13:48:01 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint10.pt\n",
      "2022-11-26 13:48:01 | INFO | hw5.seq2seq | end of epoch 10\n",
      "2022-11-26 13:48:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 11:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 14:11:28 | INFO | hw5.seq2seq | training loss: 3.9050\n",
      "2022-11-26 14:11:28 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 14:11:51 | INFO | hw5.seq2seq | example source: so we spent five weeks rebuilding the church .\n",
      "2022-11-26 14:11:51 | INFO | hw5.seq2seq | example hypothesis: 所以我們花了五週的時間重建教堂 。\n",
      "2022-11-26 14:11:51 | INFO | hw5.seq2seq | example reference: 我們花了五個星期去重建教會 。\n",
      "2022-11-26 14:11:51 | INFO | hw5.seq2seq | validation loss:\t3.6972\n",
      "2022-11-26 14:11:51 | INFO | hw5.seq2seq | BLEU = 20.17 55.6/29.5/16.4/9.5 (BP = 0.896 ratio = 0.901 hyp_len = 102907 ref_len = 114152)\n",
      "2022-11-26 14:11:51 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint11.pt\n",
      "2022-11-26 14:11:51 | INFO | hw5.seq2seq | end of epoch 11\n",
      "2022-11-26 14:11:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 12:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 14:35:19 | INFO | hw5.seq2seq | training loss: 3.8697\n",
      "2022-11-26 14:35:19 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 14:35:39 | INFO | hw5.seq2seq | example source: they're a lot like embryonic stem cells except without the controversy .\n",
      "2022-11-26 14:35:39 | INFO | hw5.seq2seq | example hypothesis: 它們就像胚胎幹細胞 , 除了沒有爭議 。\n",
      "2022-11-26 14:35:39 | INFO | hw5.seq2seq | example reference: 和胚胎幹細胞有許多相似之處只是沒有道德爭議性\n",
      "2022-11-26 14:35:39 | INFO | hw5.seq2seq | validation loss:\t3.6669\n",
      "2022-11-26 14:35:39 | INFO | hw5.seq2seq | BLEU = 20.83 56.2/30.2/16.9/10.0 (BP = 0.900 ratio = 0.905 hyp_len = 103285 ref_len = 114152)\n",
      "2022-11-26 14:35:40 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint12.pt\n",
      "2022-11-26 14:35:40 | INFO | hw5.seq2seq | end of epoch 12\n",
      "2022-11-26 14:35:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 13:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 14:59:17 | INFO | hw5.seq2seq | training loss: 3.8386\n",
      "2022-11-26 14:59:17 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 14:59:37 | INFO | hw5.seq2seq | example source: during the twilight years of this century , i believe humans will be unrecognizable in morphology and dynamics from what we are today .\n",
      "2022-11-26 14:59:37 | INFO | hw5.seq2seq | example hypothesis: 在這個世紀的兩倍之內 , 我相信人類在生物學和動力學中無法辨識人類的能力 。\n",
      "2022-11-26 14:59:37 | INFO | hw5.seq2seq | example reference: 我相信本世紀末將會出現今天的人類所無法辨識的形態和動力學 。\n",
      "2022-11-26 14:59:37 | INFO | hw5.seq2seq | validation loss:\t3.6449\n",
      "2022-11-26 14:59:37 | INFO | hw5.seq2seq | BLEU = 21.45 55.9/30.0/16.8/10.0 (BP = 0.931 ratio = 0.933 hyp_len = 106500 ref_len = 114152)\n",
      "2022-11-26 14:59:37 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint13.pt\n",
      "2022-11-26 14:59:38 | INFO | hw5.seq2seq | end of epoch 13\n",
      "2022-11-26 14:59:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 14:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 15:23:15 | INFO | hw5.seq2seq | training loss: 3.8132\n",
      "2022-11-26 15:23:15 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 15:23:38 | INFO | hw5.seq2seq | example source: so now they were three , doaa , bassem and little malek .\n",
      "2022-11-26 15:23:38 | INFO | hw5.seq2seq | example hypothesis: 所以現在他們三歲 , doaaa、bassssssem和小男子 。\n",
      "2022-11-26 15:23:38 | INFO | hw5.seq2seq | example reference: 因此現在他們是三個人:doaa、bassem和malek 。\n",
      "2022-11-26 15:23:38 | INFO | hw5.seq2seq | validation loss:\t3.6243\n",
      "2022-11-26 15:23:38 | INFO | hw5.seq2seq | BLEU = 21.55 55.6/29.9/16.7/9.9 (BP = 0.942 ratio = 0.944 hyp_len = 107704 ref_len = 114152)\n",
      "2022-11-26 15:23:38 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint14.pt\n",
      "2022-11-26 15:23:39 | INFO | hw5.seq2seq | end of epoch 14\n",
      "2022-11-26 15:23:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 15:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 15:46:56 | INFO | hw5.seq2seq | training loss: 3.7917\n",
      "2022-11-26 15:46:56 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 15:47:19 | INFO | hw5.seq2seq | example source: the patient's in the hospital because they're hurting .\n",
      "2022-11-26 15:47:19 | INFO | hw5.seq2seq | example hypothesis: 病人在醫院裡 , 因為他們正在傷害 。\n",
      "2022-11-26 15:47:19 | INFO | hw5.seq2seq | example reference: 病人住院是因為他們會痛 。\n",
      "2022-11-26 15:47:19 | INFO | hw5.seq2seq | validation loss:\t3.6171\n",
      "2022-11-26 15:47:19 | INFO | hw5.seq2seq | BLEU = 21.86 55.3/29.8/16.8/9.9 (BP = 0.957 ratio = 0.958 hyp_len = 109323 ref_len = 114152)\n",
      "2022-11-26 15:47:19 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint15.pt\n",
      "2022-11-26 15:47:19 | INFO | hw5.seq2seq | end of epoch 15\n",
      "2022-11-26 15:47:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 16:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:10:21 | INFO | hw5.seq2seq | training loss: 3.7713\n",
      "2022-11-26 16:10:21 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:10:42 | INFO | hw5.seq2seq | example source: and i said , \" yes . \"\n",
      "2022-11-26 16:10:42 | INFO | hw5.seq2seq | example hypothesis: 我說: 「 是的 。 」\n",
      "2022-11-26 16:10:42 | INFO | hw5.seq2seq | example reference: 我說: 「 有啊 。 」\n",
      "2022-11-26 16:10:42 | INFO | hw5.seq2seq | validation loss:\t3.5954\n",
      "2022-11-26 16:10:42 | INFO | hw5.seq2seq | BLEU = 21.72 56.8/30.8/17.4/10.3 (BP = 0.918 ratio = 0.921 hyp_len = 105123 ref_len = 114152)\n",
      "2022-11-26 16:10:42 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint16.pt\n",
      "2022-11-26 16:10:42 | INFO | hw5.seq2seq | end of epoch 16\n",
      "2022-11-26 16:10:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 17:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:35:25 | INFO | hw5.seq2seq | training loss: 3.7501\n",
      "2022-11-26 16:35:25 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:35:47 | INFO | hw5.seq2seq | example source: that will then become a reward for the lowerfrequency behavior .\n",
      "2022-11-26 16:35:47 | INFO | hw5.seq2seq | example hypothesis: 那會變成低頻率的獎勵 。\n",
      "2022-11-26 16:35:47 | INFO | hw5.seq2seq | example reference: 這樣的安排會鼓勵狗狗去做原本不喜歡做的事 。\n",
      "2022-11-26 16:35:47 | INFO | hw5.seq2seq | validation loss:\t3.5882\n",
      "2022-11-26 16:35:47 | INFO | hw5.seq2seq | BLEU = 21.68 57.6/31.4/17.8/10.6 (BP = 0.897 ratio = 0.902 hyp_len = 102930 ref_len = 114152)\n",
      "2022-11-26 16:35:48 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint17.pt\n",
      "2022-11-26 16:35:48 | INFO | hw5.seq2seq | end of epoch 17\n",
      "2022-11-26 16:35:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 18:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:59:14 | INFO | hw5.seq2seq | training loss: 3.7349\n",
      "2022-11-26 16:59:14 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:59:35 | INFO | hw5.seq2seq | example source: if the ancient greeks had found out about seasons in australia , they could have easily varied their myth to predict that .\n",
      "2022-11-26 16:59:35 | INFO | hw5.seq2seq | example hypothesis: 如果古希臘人在澳洲發現了季節 , 他們可能會很容易改變他們的神話 。\n",
      "2022-11-26 16:59:35 | INFO | hw5.seq2seq | example reference: 如果古希臘人發現澳洲的季節與他們不同 , 他們可以很簡單的修正他們的神話去預測並解釋它 。\n",
      "2022-11-26 16:59:35 | INFO | hw5.seq2seq | validation loss:\t3.5682\n",
      "2022-11-26 16:59:35 | INFO | hw5.seq2seq | BLEU = 21.95 57.4/31.3/17.8/10.6 (BP = 0.910 ratio = 0.914 hyp_len = 104286 ref_len = 114152)\n",
      "2022-11-26 16:59:35 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint18.pt\n",
      "2022-11-26 16:59:35 | INFO | hw5.seq2seq | end of epoch 18\n",
      "2022-11-26 16:59:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 19:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 17:23:10 | INFO | hw5.seq2seq | training loss: 3.7238\n",
      "2022-11-26 17:23:10 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 17:23:32 | INFO | hw5.seq2seq | example source: it might take high concentrations of antibodies against multiple parasite proteins .\n",
      "2022-11-26 17:23:32 | INFO | hw5.seq2seq | example hypothesis: 它可能需要高濃度的抗體對抗多種寄生蟲蛋白質 。\n",
      "2022-11-26 17:23:32 | INFO | hw5.seq2seq | example reference: 可能會需要高濃度的抗體來對抗多種寄生蟲蛋白質 。\n",
      "2022-11-26 17:23:32 | INFO | hw5.seq2seq | validation loss:\t3.5626\n",
      "2022-11-26 17:23:32 | INFO | hw5.seq2seq | BLEU = 22.16 57.0/31.2/17.7/10.6 (BP = 0.923 ratio = 0.926 hyp_len = 105672 ref_len = 114152)\n",
      "2022-11-26 17:23:32 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint19.pt\n",
      "2022-11-26 17:23:32 | INFO | hw5.seq2seq | end of epoch 19\n",
      "2022-11-26 17:23:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 20:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 17:46:52 | INFO | hw5.seq2seq | training loss: 3.7095\n",
      "2022-11-26 17:46:52 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 17:47:14 | INFO | hw5.seq2seq | example source: from 1998 to 2000 , 21 . 37 million people lost their jobs in china .\n",
      "2022-11-26 17:47:14 | INFO | hw5.seq2seq | example hypothesis: 從1998年到2000年 , 2130萬人失去了他們的工作 。\n",
      "2022-11-26 17:47:14 | INFO | hw5.seq2seq | example reference: 從1998年到2000年兩年期間 , 在中國失去工作的這些人就有2137萬人 。\n",
      "2022-11-26 17:47:14 | INFO | hw5.seq2seq | validation loss:\t3.5475\n",
      "2022-11-26 17:47:14 | INFO | hw5.seq2seq | BLEU = 22.38 57.2/31.3/17.9/10.7 (BP = 0.925 ratio = 0.927 hyp_len = 105864 ref_len = 114152)\n",
      "2022-11-26 17:47:14 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint20.pt\n",
      "2022-11-26 17:47:14 | INFO | hw5.seq2seq | end of epoch 20\n",
      "2022-11-26 17:47:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 21:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 18:10:32 | INFO | hw5.seq2seq | training loss: 3.6975\n",
      "2022-11-26 18:10:32 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 18:10:52 | INFO | hw5.seq2seq | example source: now i think it's fair to say that these types of parasites also flourish in our human markets .\n",
      "2022-11-26 18:10:52 | INFO | hw5.seq2seq | example hypothesis: 我覺得這很公平地說這些寄生蟲也會在我們的人類市場中繁殖 。\n",
      "2022-11-26 18:10:52 | INFO | hw5.seq2seq | example reference: 現在我想這麼說很公平 , 這種寄生菌也可以在人類的市場上蓬勃發展 。\n",
      "2022-11-26 18:10:52 | INFO | hw5.seq2seq | validation loss:\t3.5449\n",
      "2022-11-26 18:10:52 | INFO | hw5.seq2seq | BLEU = 22.41 57.2/31.2/17.7/10.6 (BP = 0.931 ratio = 0.933 hyp_len = 106546 ref_len = 114152)\n",
      "2022-11-26 18:10:52 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint21.pt\n",
      "2022-11-26 18:10:52 | INFO | hw5.seq2seq | end of epoch 21\n",
      "2022-11-26 18:10:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 22:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 18:34:11 | INFO | hw5.seq2seq | training loss: 3.6880\n",
      "2022-11-26 18:34:11 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 18:34:33 | INFO | hw5.seq2seq | example source: this is what we call a perfect fifth .\n",
      "2022-11-26 18:34:33 | INFO | hw5.seq2seq | example hypothesis: 這是我們稱之為 「 完美第五 」 。\n",
      "2022-11-26 18:34:33 | INFO | hw5.seq2seq | example reference: 這是我們說的完全五度 。\n",
      "2022-11-26 18:34:33 | INFO | hw5.seq2seq | validation loss:\t3.5363\n",
      "2022-11-26 18:34:33 | INFO | hw5.seq2seq | BLEU = 22.59 57.0/31.3/17.8/10.7 (BP = 0.935 ratio = 0.937 hyp_len = 106953 ref_len = 114152)\n",
      "2022-11-26 18:34:34 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint22.pt\n",
      "2022-11-26 18:34:34 | INFO | hw5.seq2seq | end of epoch 22\n",
      "2022-11-26 18:34:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 23:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 18:57:58 | INFO | hw5.seq2seq | training loss: 3.6759\n",
      "2022-11-26 18:57:58 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 18:58:20 | INFO | hw5.seq2seq | example source: this shows the sound being played at that burst frequency .\n",
      "2022-11-26 18:58:20 | INFO | hw5.seq2seq | example hypothesis: 這讓聲音在燃燒頻率上演 。\n",
      "2022-11-26 18:58:20 | INFO | hw5.seq2seq | example reference: 它呈現出這聲音是用那種脈衝頻率來播放的 。\n",
      "2022-11-26 18:58:20 | INFO | hw5.seq2seq | validation loss:\t3.5221\n",
      "2022-11-26 18:58:20 | INFO | hw5.seq2seq | BLEU = 22.66 57.1/31.4/17.9/10.7 (BP = 0.935 ratio = 0.937 hyp_len = 106989 ref_len = 114152)\n",
      "2022-11-26 18:58:21 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint23.pt\n",
      "2022-11-26 18:58:21 | INFO | hw5.seq2seq | end of epoch 23\n",
      "2022-11-26 18:58:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 24:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 19:21:52 | INFO | hw5.seq2seq | training loss: 3.6672\n",
      "2022-11-26 19:21:52 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 19:22:15 | INFO | hw5.seq2seq | example source: so , yeah , i get it , with , like , the weird appliances and then the total absence of conventional instruments and this glut of conductors , people might , you know , wonder , yeah , \" is this music ? \"\n",
      "2022-11-26 19:22:15 | INFO | hw5.seq2seq | example hypothesis: 所以 , 是的 , 我得到了它 , 像是 , 奇怪的應用 , 接著 , 完全不符合傳統的樂器 , 以及這些指揮家 , 大家可能會想 , 「 嗯 , 這是音樂嗎 ? 」\n",
      "2022-11-26 19:22:15 | INFO | hw5.seq2seq | example reference: 所以我了解 , 這些怪異的器具沒有任何常見樂器太多的指揮你可能會想 \" 這是音樂嗎 ? \"\n",
      "2022-11-26 19:22:15 | INFO | hw5.seq2seq | validation loss:\t3.5214\n",
      "2022-11-26 19:22:15 | INFO | hw5.seq2seq | BLEU = 22.75 56.9/31.2/17.7/10.7 (BP = 0.944 ratio = 0.946 hyp_len = 107976 ref_len = 114152)\n",
      "2022-11-26 19:22:15 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint24.pt\n",
      "2022-11-26 19:22:15 | INFO | hw5.seq2seq | end of epoch 24\n",
      "2022-11-26 19:22:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 25:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 19:45:33 | INFO | hw5.seq2seq | training loss: 3.6568\n",
      "2022-11-26 19:45:33 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 19:45:54 | INFO | hw5.seq2seq | example source: but that care means also you learn .\n",
      "2022-11-26 19:45:54 | INFO | hw5.seq2seq | example hypothesis: 但那就表示你也學會了 。\n",
      "2022-11-26 19:45:54 | INFO | hw5.seq2seq | example reference: 不過這樣的照顧也意味著在學習 。\n",
      "2022-11-26 19:45:54 | INFO | hw5.seq2seq | validation loss:\t3.5174\n",
      "2022-11-26 19:45:54 | INFO | hw5.seq2seq | BLEU = 22.93 57.1/31.3/17.9/10.8 (BP = 0.945 ratio = 0.946 hyp_len = 107999 ref_len = 114152)\n",
      "2022-11-26 19:45:54 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint25.pt\n",
      "2022-11-26 19:45:54 | INFO | hw5.seq2seq | end of epoch 25\n",
      "2022-11-26 19:45:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 26:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:09:12 | INFO | hw5.seq2seq | training loss: 3.6504\n",
      "2022-11-26 20:09:12 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:09:35 | INFO | hw5.seq2seq | example source: now , if that's really what it took , then why isn't it working anymore ?\n",
      "2022-11-26 20:09:35 | INFO | hw5.seq2seq | example hypothesis: 現在 , 如果這真的是需要的 , 那為什麼它不再有效 ?\n",
      "2022-11-26 20:09:35 | INFO | hw5.seq2seq | example reference: 如果真的只要這樣就可以了 , 為什麼它不再有用了 ?\n",
      "2022-11-26 20:09:35 | INFO | hw5.seq2seq | validation loss:\t3.4995\n",
      "2022-11-26 20:09:35 | INFO | hw5.seq2seq | BLEU = 22.74 57.6/31.7/18.2/11.0 (BP = 0.925 ratio = 0.928 hyp_len = 105942 ref_len = 114152)\n",
      "2022-11-26 20:09:35 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint26.pt\n",
      "2022-11-26 20:09:35 | INFO | hw5.seq2seq | end of epoch 26\n",
      "2022-11-26 20:09:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 27:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:33:18 | INFO | hw5.seq2seq | training loss: 3.6413\n",
      "2022-11-26 20:33:18 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:33:40 | INFO | hw5.seq2seq | example source: am: and this poses a problem for me , because it means i'm walking like that all night long .\n",
      "2022-11-26 20:33:40 | INFO | hw5.seq2seq | example hypothesis: 艾:這對我來說是個問題 , 因為這意味著我一夜都像那樣走 。\n",
      "2022-11-26 20:33:40 | INFO | hw5.seq2seq | example reference: 艾美:那讓我很傷腦筋 , 因為那表示我得整晚都 \" 那樣 \" 走 。\n",
      "2022-11-26 20:33:40 | INFO | hw5.seq2seq | validation loss:\t3.5002\n",
      "2022-11-26 20:33:40 | INFO | hw5.seq2seq | BLEU = 22.44 58.1/32.0/18.2/10.9 (BP = 0.911 ratio = 0.915 hyp_len = 104413 ref_len = 114152)\n",
      "2022-11-26 20:33:40 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint27.pt\n",
      "2022-11-26 20:33:40 | INFO | hw5.seq2seq | end of epoch 27\n",
      "2022-11-26 20:33:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 28:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:57:20 | INFO | hw5.seq2seq | training loss: 3.6369\n",
      "2022-11-26 20:57:20 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:57:42 | INFO | hw5.seq2seq | example source: and then count down the days and see what people ended up eating .\n",
      "2022-11-26 20:57:42 | INFO | hw5.seq2seq | example hypothesis: 然後數日 , 看看人們最後吃了什麼 。\n",
      "2022-11-26 20:57:42 | INFO | hw5.seq2seq | example reference: 接著我們倒數日子最後來看看人們最後到底吃了些甚麼\n",
      "2022-11-26 20:57:42 | INFO | hw5.seq2seq | validation loss:\t3.4913\n",
      "2022-11-26 20:57:42 | INFO | hw5.seq2seq | BLEU = 22.78 58.3/32.3/18.6/11.2 (BP = 0.912 ratio = 0.915 hyp_len = 104481 ref_len = 114152)\n",
      "2022-11-26 20:57:42 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint28.pt\n",
      "2022-11-26 20:57:42 | INFO | hw5.seq2seq | end of epoch 28\n",
      "2022-11-26 20:57:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 29:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:21:27 | INFO | hw5.seq2seq | training loss: 3.6296\n",
      "2022-11-26 21:21:27 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:21:48 | INFO | hw5.seq2seq | example source: antoinette told me , \" everyone is so proud of it . \"\n",
      "2022-11-26 21:21:48 | INFO | hw5.seq2seq | example hypothesis: atotte告訴我: 「 每個人都很自豪 。 」\n",
      "2022-11-26 21:21:48 | INFO | hw5.seq2seq | example reference: 安東尼告訴我 , 「 大家都感到很自豪 。 」\n",
      "2022-11-26 21:21:48 | INFO | hw5.seq2seq | validation loss:\t3.4910\n",
      "2022-11-26 21:21:48 | INFO | hw5.seq2seq | BLEU = 22.65 58.7/32.6/18.7/11.3 (BP = 0.897 ratio = 0.902 hyp_len = 102970 ref_len = 114152)\n",
      "2022-11-26 21:21:48 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint29.pt\n",
      "2022-11-26 21:21:48 | INFO | hw5.seq2seq | end of epoch 29\n",
      "2022-11-26 21:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 30:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:44:52 | INFO | hw5.seq2seq | training loss: 3.6243\n",
      "2022-11-26 21:44:52 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:45:14 | INFO | hw5.seq2seq | example source: this masks their scent from predators and protects them from parasites , so they can sleep soundly .\n",
      "2022-11-26 21:45:14 | INFO | hw5.seq2seq | example hypothesis: 這會讓牠們的味道從掠食者身上 , 保護牠們從寄生蟲身上 , 所以牠們可以睡覺 。\n",
      "2022-11-26 21:45:14 | INFO | hw5.seq2seq | example reference: 這招能夠遮掩牠們的香味 , 避開食肉動物 , 保護牠們遠離寄生生物 , 如此就能一夜好眠 。\n",
      "2022-11-26 21:45:14 | INFO | hw5.seq2seq | validation loss:\t3.4892\n",
      "2022-11-26 21:45:14 | INFO | hw5.seq2seq | BLEU = 22.77 58.1/32.1/18.5/11.1 (BP = 0.915 ratio = 0.918 hyp_len = 104789 ref_len = 114152)\n",
      "2022-11-26 21:45:14 | INFO | hw5.seq2seq | saved epoch checkpoint: E:\\project\\python\\ml_pratice\\checkpoints\\rnn/checkpoint30.pt\n",
      "2022-11-26 21:45:14 | INFO | hw5.seq2seq | end of epoch 30\n"
     ]
    }
   ],
   "source": [
    "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
    "try_load_checkpoint(model, optimizer, name=config.resume)\n",
    "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
    "    # train for one epoch\n",
    "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
    "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
    "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
    "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyjRwllxPjtf"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "N70Gc6smPi1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(inputs=['./checkpoints/rnn'], output='./checkpoints/rnn/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, checkpoint_upper_bound=None)\n",
      "averaging checkpoints:  ['./checkpoints/rnn\\\\checkpoint30.pt', './checkpoints/rnn\\\\checkpoint29.pt', './checkpoints/rnn\\\\checkpoint28.pt', './checkpoints/rnn\\\\checkpoint27.pt', './checkpoints/rnn\\\\checkpoint26.pt']\n",
      "Finished writing averaged checkpoint to ./checkpoints/rnn/avg_last_5_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "# averaging a few checkpoints can have a similar effect to ensemble\n",
    "checkdir=config.savedir\n",
    "!python ./fairseq/scripts/average_checkpoints.py \\\n",
    "--inputs {checkdir} \\\n",
    "--num-epoch-checkpoints 5 \\\n",
    "--output {checkdir}/avg_last_5_checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAGMiun8PnZy"
   },
   "source": [
    "## Confirm model weights used to generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "tvRdivVUPnsU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:45:17 | INFO | hw5.seq2seq | loaded checkpoint checkpoints\\rnn\\avg_last_5_checkpoint.pt: step=unknown loss=3.489190101623535 bleu=22.767399273093602\n",
      "2022-11-26 21:45:17 | INFO | hw5.seq2seq | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:45:38 | INFO | hw5.seq2seq | example source: i don't think we've even begun to discover what the basic principles are , but i think we can begin to think about them .\n",
      "2022-11-26 21:45:38 | INFO | hw5.seq2seq | example hypothesis: 我不認為我們已經開始了解基本原則是什麼 , 但我認為我們可以開始思考它們 。\n",
      "2022-11-26 21:45:38 | INFO | hw5.seq2seq | example reference: 甚至我們連最基本的原理是什麼都還未搞清楚 , 但是我們可以開始去思索這些問題 。\n",
      "2022-11-26 21:45:38 | INFO | hw5.seq2seq | validation loss:\t3.4736\n",
      "2022-11-26 21:45:38 | INFO | hw5.seq2seq | BLEU = 23.03 58.4/32.5/18.8/11.3 (BP = 0.914 ratio = 0.917 hyp_len = 104725 ref_len = 114152)\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_last.pt : latest epoch\n",
    "# checkpoint_best.pt : highest validation bleu\n",
    "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
    "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
    "validate(model, task, criterion, log_to_wandb=False)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioAIflXpPsxt"
   },
   "source": [
    "## Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "oYMxA8FlPtIq"
   },
   "outputs": [],
   "source": [
    "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
    "    task.load_dataset(split=split, epoch=1)\n",
    "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    idxs = []\n",
    "    hyps = []\n",
    "\n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "\n",
    "            # do inference\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            \n",
    "            hyps.extend(h)\n",
    "            idxs.extend(list(sample['id']))\n",
    "            \n",
    "    # sort based on the order before preprocess\n",
    "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
    "    \n",
    "    with open(outfile, \"w\", encoding = 'utf-8') as f:\n",
    "        for h in hyps:\n",
    "            f.write(h+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Le4RFWXxjmm0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:45:38 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020\\test.en-zh.en\n",
      "2022-11-26 21:45:39 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020\\test.en-zh.zh\n",
      "2022-11-26 21:45:39 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 test en-zh 4000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0414347c00143a19c8c75568f3f1acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prediction:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_prediction(model, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wvenyi6BPwnD"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z0cJE-wPzaU"
   },
   "source": [
    "# Back-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-7uPJ2CP0sm"
   },
   "source": [
    "## Train a backward translation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppGHjg2ZP3sV"
   },
   "source": [
    "1. Switch the source_lang and target_lang in **config** \n",
    "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
    "3. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waTGz29UP6WI"
   },
   "source": [
    "## Generate synthetic data with backward model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIeTsPexP8FL"
   },
   "source": [
    "### Download monolingual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7N4QlsbP8fh"
   },
   "outputs": [],
   "source": [
    "mono_dataset_name = 'mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "396saD9-QBPY"
   },
   "outputs": [],
   "source": [
    "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
    "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "urls = (\n",
    "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted_zh_corpus.deduped.gz\"\n",
    ")\n",
    "file_names = (\n",
    "    'ted_zh_corpus.deduped.gz',\n",
    ")\n",
    "\n",
    "for u, f in zip(urls, file_names):\n",
    "    path = mono_prefix/f\n",
    "    if not path.exists():\n",
    "        else:\n",
    "            !wget {u} -O {path}\n",
    "    else:\n",
    "        print(f'{f} is exist, skip downloading')\n",
    "    if path.suffix == \".tgz\":\n",
    "        !tar -xvf {path} -C {prefix}\n",
    "    elif path.suffix == \".zip\":\n",
    "        !unzip -o {path} -d {prefix}\n",
    "    elif path.suffix == \".gz\":\n",
    "        !gzip -fkd {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOVQRHzGQU4-"
   },
   "source": [
    "### TODO: clean corpus\n",
    "\n",
    "1. remove sentences that are too long or too short\n",
    "2. unify punctuation\n",
    "\n",
    "hint: you can use clean_s() defined above to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIYmxfUOQSov"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jegH0bvMQVmR"
   },
   "source": [
    "### TODO: Subword Units\n",
    "\n",
    "Use the spm model of the backward model to tokenize the data into subword units\n",
    "\n",
    "hint: spm model is located at DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqgR4uUMQZGY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a65glBVXQZiE"
   },
   "source": [
    "### Binarize\n",
    "\n",
    "use fairseq to binarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b803qA5aQaEu"
   },
   "outputs": [],
   "source": [
    "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
    "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
    "tgt_dict_file = src_dict_file\n",
    "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
    "if binpath.exists():\n",
    "    print(binpath, \"exists, will not overwrite!\")\n",
    "else:\n",
    "    !python -m fairseq_cli.preprocess\\\n",
    "        --source-lang 'zh'\\\n",
    "        --target-lang 'en'\\\n",
    "        --trainpref {monopref}\\\n",
    "        --destdir {binpath}\\\n",
    "        --srcdict {src_dict_file}\\\n",
    "        --tgtdict {tgt_dict_file}\\\n",
    "        --workers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smA0JraEQdxz"
   },
   "source": [
    "### TODO: Generate synthetic data with backward model\n",
    "\n",
    "Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
    "\n",
    "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
    "\n",
    "then you can use 'generate_prediction(model, task, split=\"split_name\")' to generate translation prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvaOVHeoQfkB"
   },
   "outputs": [],
   "source": [
    "# Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
    "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
    "!cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFEkxPu-Qhlc"
   },
   "outputs": [],
   "source": [
    "# hint: do prediction on split='mono' to create prediction_file\n",
    "# generate_prediction( ... ,split=... ,outfile=... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn4XeawpQjLk"
   },
   "source": [
    "### TODO: Create new dataset\n",
    "\n",
    "1. Combine the prediction data with monolingual data\n",
    "2. Use the original spm model to tokenize data into Subword Units\n",
    "3. Binarize data with fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R35JTaTQjkm"
   },
   "outputs": [],
   "source": [
    "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
    "# \n",
    "# hint: tokenize prediction_file with the spm model\n",
    "# spm_model.encode(line, out_type=str)\n",
    "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
    "#\n",
    "# hint: use fairseq to binarize these two files again\n",
    "# binpath = Path('./DATA/data-bin/synthetic')\n",
    "# src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
    "# tgt_dict_file = src_dict_file\n",
    "# monopref = ./DATA/rawdata/mono/mono.tok # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
    "# if binpath.exists():\n",
    "#     print(binpath, \"exists, will not overwrite!\")\n",
    "# else:\n",
    "#     !python -m fairseq_cli.preprocess\\\n",
    "#         --source-lang 'zh'\\\n",
    "#         --target-lang 'en'\\\n",
    "#         --trainpref {monopref}\\\n",
    "#         --destdir {binpath}\\\n",
    "#         --srcdict {src_dict_file}\\\n",
    "#         --tgtdict {tgt_dict_file}\\\n",
    "#         --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSkse1tyQnsR"
   },
   "outputs": [],
   "source": [
    "# create a new dataset from all the files prepared above\n",
    "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
    "\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
    "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVdxVGO3QrSs"
   },
   "source": [
    "Created new dataset \"ted2020_with_mono\"\n",
    "\n",
    "1. Change the datadir in **config** (\"./DATA/data-bin/ted2020_with_mono\")\n",
    "2. Switch back the source_lang and target_lang in **config** (\"en\", \"zh\")\n",
    "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-bt\")\n",
    "3. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CZU2beUQtl3"
   },
   "source": [
    "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
    "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
    "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
    "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
    "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
    "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
    "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
    "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
    "9. https://ithelp.ithome.com.tw/articles/10233122\n",
    "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "11. https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nKb4u67-sT_Z",
    "n1rwQysTsdJq",
    "59si_C0Wsms7",
    "oOpG4EBRLwe_",
    "6ZlE_1JnMv56",
    "UDAPmxjRNEEL",
    "ce5n4eS7NQNy",
    "rUB9f1WCNgMH",
    "VFJlkOMONsc6",
    "Gt1lX3DRO_yU",
    "BAGMiun8PnZy",
    "JOVQRHzGQU4-",
    "jegH0bvMQVmR",
    "a65glBVXQZiE",
    "smA0JraEQdxz",
    "Jn4XeawpQjLk"
   ],
   "name": "HW05.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
